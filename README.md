# Task_05_Descriptive_Stats
# LLM Prompt Engineering Research

This project explores **prompt engineering techniques** to evaluate the reasoning, accuracy, and adaptability of **large language models (LLMs)**.  
The study involves giving the same dataset to **three LLMs**: **ChatGPT**, **Perplexity**, and **Claude**, and analyzing their responses to a series of natural language questions and analytical prompts.

## Dataset

The dataset used for this research is the **2025 Syracuse University Womenâ€™s Lacrosse Overall Statistics** file.  
It contains:

- **Team performance metrics** (wins/losses, goals per game, shot accuracy)
- **Game-by-game results** (dates, opponents, scores, and locations)
- **Player-level statistics** (goals, assists, ground balls, turnovers)
- **Goalie performance** (saves, goals against average, save percentage)
- **Period-wise breakdowns** of shots, goals, and saves

This dataset will be provided to each LLM, and the models will be asked a variety of questions, ranging from **basic descriptive queries** (e.g., *Who scored the most goals this season?*) to **analytical insights** (e.g., *Which player showed the most improvement throughout the season?*).
